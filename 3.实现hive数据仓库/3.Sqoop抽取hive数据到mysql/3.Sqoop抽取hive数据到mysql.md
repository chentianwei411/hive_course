### sqoop导出操作
>>前面第一部分讲了Sqoop从mysql导入数据到hive中，既然有导入那相对也有导出操作

**知识点**
Sqoop Export   导出
将数据从Hadoop（如hive等）导入关系型数据库导中
- 步骤1：Sqoop与数据库Server通信，获取数据库表的元数据信息；
- 步骤2：并行导入数据：
- 将Hadoop上文件划分成若干个split；
- 每个split由一个Map Task进行数据导入

---

先看下之前第二部分做聚合表的数据，我们这次以导出每日环比表(dw_order_by_day)为例子

![](https://upload-images.jianshu.io/upload_images/12312683-047609a3ff55f9aa.png)




**1.首先在百度云服务器上的mysql数据库的adventure_dw库建表（dw_order_by_day）**
```
CREATE TABLE `dw_order_by_day` (
   `create_date` date DEFAULT NULL,
   `sum_amount` double DEFAULT NULL,
   `sum_order` bigint(20) DEFAULT NULL,
   `amount_div_order` double DEFAULT NULL,
   `sum_amount_goal` double DEFAULT NULL,
   `sum_order_goal` double DEFAULT NULL,
   `is_current_year` int(11) DEFAULT NULL,
   `is_last_year` int(11) DEFAULT NULL,
   `is_yesterday` int(11) DEFAULT NULL,
   `is_today` int(11) DEFAULT NULL,
   `is_current_month` int(11) DEFAULT NULL,
   `is_current_quarter` int(11) DEFAULT NULL,
   `is_21_day` int(11) DEFAULT NULL,
   `amount_diff` double DEFAULT NULL
 ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_unicode_ci
```


**2开始进行sqoop从hive数据库抽取数据到mysql数据库**
```
sqoop export \
--connect "jdbc:mysql://106.13.128.83:3306/adventure_dw?useUnicode=true&characterEncoding=utf-8&zeroDateTimeBehavior=convertToNull&tinyInt1isBit=false&dontTrackOpenResources=true&defaultfetchSize=50000&useCursorfetch=true" \
--username frogdata001\  ##数据库账号 
--password Frogdata!123 \##数据库密码
--table dw_order_by_day \  ##mysql数据库建好的表  
--export-dir /user/hive/warehouse/ods.db/dw_order_by_day \  #hive数据库数据路径，这个用show create table ods.dw_order_by_day 查hive表的路径 
--columns ##抽取的列 create_date,is_current_year,is_last_year,is_yesterday,is_today,is_current_month,is_current_quart,sum_amount,order_count \
--fields-terminated-by '\001' \ ##hive中被导出的文件字段的分隔符
```
### **注：**
- **以上的命令中后面的##部分是注释，执行的时候需要删掉；另外，命令的所有内容不能换行，只能一行才能执行。**
- 一般是写到shell脚本上，在linux 系统中运行即可

- 命令执行完后，再去观mysql中的数据表，是不是已经有数据存在了！

- Sqoop的更多详细参数请看 https://www.cnblogs.com/alexzhang92/p/10927884.html

---

>我们需要从Hive里把**时间_地区_产品聚合 表**（dw_customer_order）,**每日环比表**（dw_order_by_day）、**当日维度表**（dw_amount_diff）分别通过sqoop把这些表的数据给迁入到mysql的adventure_dw库中，目前我们已经做好了**时间_地区_产品聚合 表**（dw_customer_order）,**每日环比表**（dw_order_by_day）的导出代码，剩下的**当日维度表**（dw_amount_diff）需要独立完成，可参考文档里的代码。
